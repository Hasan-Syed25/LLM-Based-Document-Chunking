{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ **Notebook Overview**\n",
        "\n",
        "This notebook extracts structured content from a **DOCX document** using **OpenAI‚Äôs GPT-4o model**. It is designed to address a critical problem in document chunking: when clauses or sections with the same heading get divided into multiple chunks, retrieval systems‚Äîeven when using hybrid search‚Äîmay return only a subset of those chunks. As a result, the language model may hallucinate or generate incomplete responses because it lacks the full context of the clause.\n",
        "\n",
        "### ‚úÖ **Problems This Notebook Solves**\n",
        "\n",
        "- **Ensures Cohesive Chunking:**  \n",
        "  Prevents a single clause or heading from being split into unrelated pieces, ensuring that the entire clause is treated as a unified segment.\n",
        "\n",
        "- **Improves Retrieval Accuracy:**  \n",
        "  By preserving the integrity of each clause, the system ensures that retrieval methods return complete information, reducing the risk of hallucinations.\n",
        "\n",
        "- **Extracts a Structured Overview:**  \n",
        "  Organizes text into a structured JSON format that captures logical sections and cross-references for easier downstream processing.\n",
        "\n",
        "- **Converts DOCX to TXT:**  \n",
        "  Facilitates text processing by converting DOCX files into plain text, which is then segmented intelligently.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Required Parameters**\n",
        "\n",
        "1. **`input_path`** ‚Üí Path to the input DOCX file üìÑ  \n",
        "2. **`output_dir`** ‚Üí Directory where the extracted outputs will be saved üìÇ  \n",
        "3. **`api_key`** ‚Üí OpenAI API Key üîë  \n",
        "4. **`endpoint`** ‚Üí Azure OpenAI API Endpoint üåê  \n",
        "5. **`deployment_name`** ‚Üí Name of the GPT model deployment üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîÑ Processing Steps**\n",
        "\n",
        "1Ô∏è‚É£ **Convert DOCX to TXT** (üìÑ ‚û°Ô∏è üìú)  \n",
        "2Ô∏è‚É£ **Extract the Structured Overview:**  \n",
        "   - Segment the document into logical sections while keeping entire clauses intact. üìë  \n",
        "3Ô∏è‚É£ **Parse and Save the Overview/Segments into a JSON File** üóÑÔ∏è  \n",
        "4Ô∏è‚É£ **Extract Each Clause with Its Cross-References:**  \n",
        "   - Ensure that all parts of a clause are grouped together for coherent retrieval. üîç  \n",
        "5Ô∏è‚É£ **Save the Final Structured Data into JSON** ‚úÖ  \n",
        "\n",
        "---\n",
        "\n",
        "By maintaining the complete structure of each clause, this notebook reduces the risk of the language model hallucinating due to missing context, leading to more accurate and reliable responses during retrieval."
      ],
      "metadata": {
        "id": "X-l5T-BjvXIT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzdm4GTpYeU4"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "%pip install -q openai\n",
        "!apt install libreoffice"
      ],
      "metadata": {
        "id": "hmZba7t9hq9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs"
      ],
      "metadata": {
        "id": "xjcirL4q122l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "# üìå User Inputs\n",
        "Input_File_Path = \"/content/Sample.docx\"  # @param {type:\"string\", placeholder:\"Enter document path\"}\n",
        "Output_Dir = \"/content/clause_chunks_sample_doc\"  # @param {type:\"string\", placeholder:\"Enter output directory\"}\n",
        "API_Key    = \"\" # @param {type:\"string\", placeholder:\"Enter OpenAI API Key\"}\n",
        "Endpoint   = \"\" # @param {type:\"string\", placeholder:\"Enter OpenAI API Endpoint\"}\n",
        "Deployment_Name = \"\" # @param {type:\"string\", placeholder:\"Enter deployment name\"}\n",
        "# üéØ Setting Up OpenAI Credentials\n",
        "oc = openai.AsyncAzureOpenAI(\n",
        "    azure_endpoint=Endpoint,\n",
        "    azure_deployment=Deployment_Name,\n",
        "    api_key=API_Key,\n",
        "    api_version=\"2024-08-01-preview\"\n",
        ")\n",
        "\n",
        "# üìÑ Convert DOCX to TXT\n",
        "async def convert_docx_to_txt(Input_File_Path: str, Output_Dir: str) -> str:\n",
        "    print(\"\\nüìÇ Converting DOCX to TXT...\")\n",
        "    command = f'soffice --headless --convert-to txt:Text \"{Input_File_Path}\" --outdir \"{Output_Dir}\"'\n",
        "    process = await asyncio.create_subprocess_shell(command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
        "    stdout, stderr = await process.communicate()\n",
        "\n",
        "    if process.returncode != 0:\n",
        "        print(\"‚ùå Error during conversion!\")\n",
        "        raise Exception(f\"Error: {stderr.decode().strip()}\")\n",
        "\n",
        "    txt_path = os.path.join(Output_Dir, os.path.splitext(os.path.basename(Input_File_Path))[0] + \".txt\")\n",
        "    print(f\"‚úÖ Conversion complete! TXT file saved at: {txt_path}\")\n",
        "    return txt_path\n",
        "\n",
        "# üìú Load document content\n",
        "def load_document(file_path):\n",
        "    print(\"\\nüìñ Loading document content...\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    print(\"‚úÖ Document loaded successfully!\")\n",
        "    return content\n",
        "\n",
        "# üèõÔ∏è Define Pydantic schema for structured output\n",
        "class Clause(BaseModel):\n",
        "    clause_header: str\n",
        "    subsection_number: List[str]\n",
        "    reference_clause_header: List[str]\n",
        "\n",
        "class TableOfContents(BaseModel):\n",
        "    clauses: List[Clause]\n",
        "\n",
        "# üìë Extract Table of Contents (ToC) using GPT\n",
        "async def extract_toc(text):\n",
        "    print(\"\\nü§ñ Extracting Table of Contents using GPT-4o...\")\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an advanced Table of Contents (ToC) extractor AI. \"\n",
        "        \"Your task is to analyze the provided legal document and return a JSON object following this schema: \"\n",
        "        \"{'clause_header': str, 'subsections': List[str], 'reference_clause_header': List[str]}. \"\n",
        "        \"Extract all clauses, sections, subsections, schedules, annexes, and appendices.\"\n",
        "        \"For each clause, include any references to other clause_header in the 'reference_clause_header' field. Use clause_header for this purpose\"\n",
        "        \"If you are unable to extract the ToC, return an empty JSON object. \"\n",
        "        \"Ensure accuracy and completeness in the ToC extraction.\"\n",
        "        \"## Important Note: Never Extract ToC from the Table of Contents itself. Instead extract from the main content of the document.\"\n",
        "        \"- Return the Clause_Headers with exact grammar and hierarchy as they appear in the document even if it is wrong.\"\n",
        "    )\n",
        "\n",
        "    response = await oc.beta.chat.completions.parse(\n",
        "        model=Deployment_Name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Extract the Table of Contents from the following document:\\n\\n{text}\"}\n",
        "        ],\n",
        "        response_format=TableOfContents,\n",
        "        max_tokens=3000,\n",
        "        temperature=0.1,\n",
        "        top_p=1.0\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ ToC extraction complete!\")\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# üîç Extract clause-wise chunks\n",
        "def extract_clauses(document, toc):\n",
        "    print(\"\\nüìú Extracting clauses from the document...\")\n",
        "    clause_chunks = {}\n",
        "    last_end = 0\n",
        "\n",
        "    for i, clause in enumerate(toc[\"clauses\"]):\n",
        "        clause_header = clause[\"clause_header\"].strip()\n",
        "        match_header = re.match(r'^(\\d+)\\s+(.*)$', clause_header)\n",
        "        clause_number, clause_title = (match_header.groups() if match_header else (\"\", clause_header))\n",
        "\n",
        "        exact_pattern = rf\"(?m)^\\s*{re.escape(clause_header)}\\s*[-:]*\"\n",
        "        fallback_pattern = rf\"(?m)^\\s*{re.escape(clause_number)}\\s*{re.escape(clause_title)}[-:]*\" if clause_number else exact_pattern\n",
        "        end_pattern = r\"\\Z\" if i + 1 == len(toc[\"clauses\"]) else rf\"(?=^\\s*{re.escape(toc['clauses'][i+1]['clause_header'])})\"\n",
        "\n",
        "        exact_match = re.compile(rf\"{exact_pattern}(.*?){end_pattern}\", re.MULTILINE | re.DOTALL | re.IGNORECASE)\n",
        "        fallback_match = re.compile(rf\"{fallback_pattern}(.*?){end_pattern}\", re.MULTILINE | re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        matches = exact_match.finditer(document, last_end)\n",
        "        best_match = max(matches, key=lambda m: len(m.group(1).strip()), default=None)\n",
        "\n",
        "        if not best_match:\n",
        "            matches = fallback_match.finditer(document, last_end)\n",
        "            best_match = max(matches, key=lambda m: len(m.group(1).strip()), default=None)\n",
        "\n",
        "        clause_chunks[f\"Chunk_{i+1}\"] = {\n",
        "            \"clause_header\": clause_header,\n",
        "            \"content\": best_match.group(1).strip() if best_match else \"[No content found]\",\n",
        "            \"reference_clauses\": clause.get(\"reference_clause_header\", [])\n",
        "        }\n",
        "\n",
        "        if best_match:\n",
        "            last_end = best_match.end()\n",
        "\n",
        "    print(\"‚úÖ Clause extraction complete!\")\n",
        "    return clause_chunks\n",
        "\n",
        "# üìù Save JSON file\n",
        "def save_to_json(data, output_path):\n",
        "    json_data = json.loads(data) if isinstance(data, str) else data\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(json_data, file, indent=4, ensure_ascii=False)\n",
        "    print(f\"‚úÖ JSON saved at: {output_path}\")\n",
        "\n",
        "# üöÄ Main execution\n",
        "async def main():\n",
        "    try:\n",
        "        txt_path = await convert_docx_to_txt(Input_File_Path, Output_Dir)\n",
        "        document_text = load_document(txt_path)\n",
        "\n",
        "        toc = await extract_toc(document_text)\n",
        "        save_to_json(toc, os.path.join(Output_Dir, \"toc.json\"))\n",
        "\n",
        "        toc = json.loads(toc)\n",
        "        clause_chunks = extract_clauses(document_text, toc)\n",
        "        save_to_json(clause_chunks, os.path.join(Output_Dir, \"clause_chunks.json\"))\n",
        "\n",
        "        print(\"\\nüéâ **Processing complete! All JSON files are saved successfully.**\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlenxYtNgZlW",
        "outputId": "21dd38c3-7fe0-4184-ea84-eb80c2a81596"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÇ Converting DOCX to TXT...\n",
            "‚úÖ Conversion complete! TXT file saved at: /content/clause_chunks_sample_doc/Sample.txt\n",
            "\n",
            "üìñ Loading document content...\n",
            "‚úÖ Document loaded successfully!\n",
            "\n",
            "ü§ñ Extracting Table of Contents using GPT-4o...\n",
            "‚úÖ ToC extraction complete!\n",
            "‚úÖ JSON saved at: /content/clause_chunks_sample_doc/toc.json\n",
            "\n",
            "üìú Extracting clauses from the document...\n",
            "‚úÖ Clause extraction complete!\n",
            "‚úÖ JSON saved at: /content/clause_chunks_sample_doc/clause_chunks.json\n",
            "\n",
            "üéâ **Processing complete! All JSON files are saved successfully.**\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}